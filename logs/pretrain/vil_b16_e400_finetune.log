06-12 11:13:06 I ------------------
06-12 11:13:06 I stage_id: 6x5zxnr9
06-12 11:13:06 I python main_train.py --hp examples/vislstm/yamls/deit3/finetune/run/dediwjhv_e5.yaml
06-12 11:13:06 I ------------------
06-12 11:13:06 I VERSION CHECK
06-12 11:13:06 I executable: .../bin/python
06-12 11:13:06 I python version: 3.9.19
06-12 11:13:06 I torch version: 2.2.2+cu121
06-12 11:13:06 I torch.cuda version: 12.1
06-12 11:13:06 I torchvision.version: 0.17.2+cu121
06-12 11:13:07 I torchmetrics version: 1.4.0
06-12 11:13:07 I kappaschedules version: 0.0.31
06-12 11:13:07 I kappamodules version: 0.1.70
06-12 11:13:07 I ------------------
06-12 11:13:07 I SYSTEM INFO
06-12 11:13:07 I host name: ...
06-12 11:13:07 I OS: ...
06-12 11:13:07 I OS version: ...
06-12 11:13:09 I CUDA version: 12.5
06-12 11:13:09 I current commit hash: f8a5ae1106332904ad1c17c5e7b5ac88b49b1602
06-12 11:13:09 I latest git tag:
06-12 11:13:09 I initialized process rank=0 local_rank=0 pid=1437468 hostname=...
06-12 11:13:09 I total_cpu_count: 16
06-12 11:13:09 I ------------------
06-12 11:13:09 I STATIC CONFIG
06-12 11:13:09 I account_name: ...
06-12 11:13:09 I output_path: .../save
06-12 11:13:09 I local_dataset_path: /tmp
06-12 11:13:09 I available space in local_dataset_path:
06-12 11:13:09 I Filesystem      Size  Used Avail Use% Mounted on
06-12 11:13:09 I tmpfs...
06-12 11:13:09 I ------------------
06-12 11:13:09 I CLI ARGS
06-12 11:13:09 I hp: examples/vislstm/yamls/deit3/finetune/run/dediwjhv_e5.yaml
06-12 11:13:09 I accelerator: gpu
06-12 11:13:09 I testrun: False
06-12 11:13:09 I minmodelrun: False
06-12 11:13:09 I mindatarun: False
06-12 11:13:09 I mindurationrun: False
06-12 11:13:09 I static_config_uri: static_config.yaml
06-12 11:13:09 I ------------------
06-12 11:13:09 I DIST CONFIG
06-12 11:13:09 I rank: 0
06-12 11:13:09 I local_rank: 0
06-12 11:13:09 I world_size: 16
06-12 11:13:09 I nodes: 2
06-12 11:13:09 I backend: nccl
06-12 11:13:09 I slurm job id: ...
06-12 11:13:09 I hostnames: ...
06-12 11:13:09 I ------------------
master_factory_base_path: vislstm
stage_name: in1k
datasets:
  train:
    kind: imagenet1k
    split: train
    sample_wrappers:
    - kind: x_transform_wrapper
      transform:
      - kind: random_resized_crop
        size: 224
        scale:
        - 0.08
        - 1.0
        interpolation: bicubic
      - kind: random_horizontal_flip
      - kind: transforms.three_augment
        blur_sigma:
        - 0.1
        - 2.0
      - kind: color_jitter
        brightness: 0.3
        contrast: 0.3
        saturation: 0.3
        hue: 0.0
      - kind: imagenet1k_norm
    - kind: one_hot_wrapper
    collators:
    - kind: mix_collator
      mixup_alpha: 0.8
      cutmix_alpha: 1.0
      mixup_p: 0.5
      cutmix_p: 0.5
      apply_mode: batch
      lamb_mode: batch
      shuffle_mode: flip
  val:
    kind: imagenet1k
    split: val
    sample_wrappers:
    - kind: x_transform_wrapper
      transform:
      - kind: resize
        size: 224
        interpolation: bicubic
      - kind: center_crop
        size: 224
      - kind: imagenet1k_norm
model:
  initializers:
  - kind: previous_run_initializer
    stage_id: dediwjhv
    stage_name: in1k
    model_name: vislstm
    checkpoint: last
    use_checkpoint_kwargs: true
  optim:
    kind: adamw
    lr: 1.0e-05
    betas:
    - 0.9
    - 0.999
    weight_decay: 0.05
    schedule:
      kind: linear_warmup_cosine_decay_schedule
      warmup_epochs: 1
      end_value: 1.0e-06
    lr_scaler:
      kind: linear_lr_scaler
      divisor: 1024
trainer:
  kind: classification_trainer
  precision: bfloat16
  backup_precision: float16
  max_epochs: 5
  effective_batch_size: 1024
  log_every_n_epochs: 1
  callbacks:
  - kind: checkpoint_callback
  - kind: checkpoint_callback
    every_n_epochs: 10
    save_latest_weights: true
    save_latest_optim: true
  - kind: offline_accuracy_callback
    every_n_epochs: 1
    dataset_key: val
06-12 11:13:09 I copied unresolved hp to .../hp_unresolved.yaml
06-12 11:13:09 I dumped resolved hp to .../hp_resolved.yaml
06-12 11:13:09 I ------------------
06-12 11:13:09 I training stage 'in1k'
06-12 11:13:09 I using different seeds per process (seed+rank)
06-12 11:13:09 I set seed to 0
06-12 11:13:09 I ------------------
06-12 11:13:09 I initializing datasets
06-12 11:13:09 I initializing train
06-12 11:13:09 I extracting 1000 zips from ... to ...
06-12 11:13:54 I finished copying data from global to local
06-12 11:13:56 I instantiating sample_wrapper x_transform_wrapper
06-12 11:13:56 I instantiating sample_wrapper one_hot_wrapper
06-12 11:13:56 I initializing val
06-12 11:13:56 I extracting 1000 zips from ... to ...
06-12 11:13:58 I finished copying data from global to local
06-12 11:13:58 I instantiating sample_wrapper x_transform_wrapper
06-12 11:13:58 I ------------------
06-12 11:13:58 I initializing trainer
06-12 11:13:58 I using precision: torch.bfloat16 (desired=bfloat16 backup=float16)
06-12 11:13:58 I main_sampler: DistributedSampler(num_repeats=1, shuffle=True)
06-12 11:13:58 I ------------------
06-12 11:13:58 I creating model
06-12 11:13:58 I input_shape: (3, 224, 224)
06-12 11:13:59 I loaded model kwargs from .../in1k/dediwjhv/checkpoints/vislstm cp=last model.th
06-12 11:13:59 I loaded model kwargs: {'patch_size': 16, 'dim': 768, 'depth': 24, 'bidirectional': False, 'alternation': 'bidirectional', 'conv1d_kernel_size': 3, 'use_conv2d': True, 'bias': True, 'pos_embed_mode': 'learnable', 'drop_path_rate': 0.2, 'drop_path_decay': False, 'mode': 'classifier', 'pooling': {'kind': 'bilateral', 'aggregate': 'flatten'}, 'kind': 'models.single.vislstm'}
06-12 11:13:59 I postprocessed checkpoint kwargs:
initializers:
- kind: previous_run_initializer
  stage_id: dediwjhv
  stage_name: in1k
  model_name: vislstm
  checkpoint: last
optim:
  kind: adamw
  lr: 1.0e-05
  betas:
  - 0.9
  - 0.999
  weight_decay: 0.05
  schedule:
    kind: linear_warmup_cosine_decay_schedule
    warmup_epochs: 1
    end_value: 1.0e-06
  lr_scaler:
    kind: linear_lr_scaler
    divisor: 1024
patch_size: 16
dim: 768
depth: 24
bidirectional: false
alternation: bidirectional
conv1d_kernel_size: 3
use_conv2d: true
bias: true
pos_embed_mode: learnable
drop_path_rate: 0.2
drop_path_decay: false
mode: classifier
pooling:
  kind: bilateral
  aggregate: flatten
06-12 11:13:59 I pos_embed.is_learnable=True
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
mLSTMLayerConfig(proj_factor=2.0, round_proj_up_dim_up=True, round_proj_up_to_multiple_of=64, _proj_up_dim=1536, conv1d_kernel_size=3, qkv_proj_blocksize=4, num_heads=4, bidirectional=False, quaddirectional=False, sharedirs=False, alternation='bidirectional', layerscale=None, use_conv2d=True, use_v_conv=False, share_conv=True, embedding_dim=768, bias=True, dropout=0.0, context_length=196, _num_blocks=1, _inner_embedding_dim=1536)
06-12 11:14:00 I drop_path_rate: 0.2
06-12 11:14:00 I model:
VisLSTM(
  (pooling): Bilateral(aggregate=flatten)
  (patch_embed): VitPatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_embed): VitPosEmbed2d()
  (xlstm): xLSTMBlockStack(
    (blocks): ModuleList(
      (0-23): 24 x mLSTMBlock(
        (drop_path1): DropPath(drop_prob=0.200)
        (xlstm_norm): LayerNorm()
        (xlstm): mLSTMLayer(
          (proj_up): Linear(in_features=768, out_features=3072, bias=True)
          (q_proj): LinearHeadwiseExpand(in_features=1536, num_heads=384, expand_factor_up=1, bias=True, trainable_weight=True, trainable_bias=True, )
          (k_proj): LinearHeadwiseExpand(in_features=1536, num_heads=384, expand_factor_up=1, bias=True, trainable_weight=True, trainable_bias=True, )
          (v_proj): LinearHeadwiseExpand(in_features=1536, num_heads=384, expand_factor_up=1, bias=True, trainable_weight=True, trainable_bias=True, )
          (conv1d): SequenceConv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
          (conv_act_fn): SiLU()
          (mlstm_cell): mLSTMCell(
            (igate): Linear(in_features=4608, out_features=4, bias=True)
            (fgate): Linear(in_features=4608, out_features=4, bias=True)
            (outnorm): MultiHeadLayerNorm()
          )
          (ogate_act_fn): SiLU()
          (proj_down): Linear(in_features=1536, out_features=768, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (layerscale): Identity()
        )
      )
    )
    (post_blocks_norm): LayerNorm()
  )
  (head): Sequential(
    (0): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
    (1): Linear(in_features=1536, out_features=1000, bias=True)
  )
)
06-12 11:14:00 I vislstm initialize optimizer
06-12 11:14:00 I base lr: 1e-5
06-12 11:14:00 I scaled lr: 1e-5
06-12 11:14:00 I lr_scaler=LinearLrScaler(divisor=1024)
06-12 11:14:00 I lr_scale_factor=1024
06-12 11:14:00 I exclude_bias_from_wd=True exclude_norm_from_wd=True param_group_modifiers=[WeightDecayByNameModifier(name=pos_embed.embed)]
06-12 11:14:00 I using 2 param groups:
06-12 11:14:00 I len(params)=194
06-12 11:14:00 I weight_decay=0.0 len(params)=319
06-12 11:14:00 I interpolate pos_embed: torch.Size([1, 12, 12, 768]) -> torch.Size([1, 14, 14, 768])
06-12 11:14:00 I loaded weights of vislstm from .../in1k/dediwjhv/checkpoints/vislstm cp=last model.th
06-12 11:14:00 I added default DatasetStatsCallback
06-12 11:14:00 I added default ParamCountCallback
06-12 11:14:00 I added default CopyPreviousConfigCallback
06-12 11:14:00 I added default CopyPreviousSummaryCallback
06-12 11:14:00 I added default ProgressCallback(every_n_epochs=1)
06-12 11:14:00 I added default TrainTimeCallback(every_n_epochs=1)
06-12 11:14:00 I added default OnlineLossCallback(every_n_epochs=1)
06-12 11:14:00 I added default LrCallback(every_n_updates=50)
06-12 11:14:00 I added default FreezerCallback(every_n_updates=50)
06-12 11:14:00 I added default OnlineLossCallback(every_n_updates=50)
06-12 11:14:00 I replacing BatchNorm layers with SyncBatchNorm
06-12 11:14:00 I torch.compile not used (use_torch_compile == False)
06-12 11:14:00 I ------------------
06-12 11:14:00 I PREPARE TRAINER
06-12 11:14:00 I calculating batch_size and accumulation_steps (effective_batch_size=1024)
06-12 11:14:00 I found multi-node setting -> disable automatic batchsize (occasionally hangs)
06-12 11:14:00 I train_batches per epoch: 1251 (world_size=16 batch_size=64)
06-12 11:14:00 I initializing dataloader
06-12 11:14:00 I OfflineAccuracyCallback(every_n_epochs=1) registered InterleavedSamplerConfig(every_n_epochs=1) dataset_mode='x class'
06-12 11:14:01 I created dataloader (batch_size=64 num_workers=15 pin_memory=True total_cpu_count=16 prefetch_factor=2)
06-12 11:14:01 I concatenated dataset properties:
06-12 11:14:01 I - mode='index x class' len=1281167 root_dataset=<examples.vislstm.datasets.imagenet1k.Imagenet1k object at 0x14e62c0962b0>
06-12 11:14:01 I - mode='x class' len=50000 root_dataset=<examples.vislstm.datasets.imagenet1k.Imagenet1k object at 0x14e59f62cd00>
06-12 11:14:01 I ------------------
06-12 11:14:01 I BEFORE TRAINING
06-12 11:14:01 I train: 1281167 samples
06-12 11:14:01 I val: 50000 samples
06-12 11:14:01 I parameter counts (trainable | frozen)
06-12 11:14:01 I 89,263,528 | 0 | vislstm
06-12 11:14:01 I estimated checkpoint size: 1.0GB
06-12 11:14:01 I estimated weight checkpoint size: 357.0MB
06-12 11:14:01 I estimated optim checkpoint size: 714.1MB
06-12 11:14:01 I estimated size for 1 checkpoints: 357.0MB
06-12 11:14:01 I estimated checkpoint size: 1.0GB
06-12 11:14:01 I estimated weight checkpoint size: 357.0MB
06-12 11:14:01 I estimated optim checkpoint size: 714.1MB
06-12 11:14:01 I estimated size for 1 checkpoints: 357.0MB
06-12 11:14:01 I ------------------
06-12 11:14:01 I DatasetStatsCallback
06-12 11:14:01 I ParamCountCallback
06-12 11:14:01 I CopyPreviousConfigCallback
06-12 11:14:01 I CopyPreviousSummaryCallback
06-12 11:14:01 I ProgressCallback(every_n_epochs=1)
06-12 11:14:01 I TrainTimeCallback(every_n_epochs=1)
06-12 11:14:01 I OnlineLossCallback(every_n_epochs=1)
06-12 11:14:01 I LrCallback(every_n_updates=50)
06-12 11:14:01 I FreezerCallback(every_n_updates=50)
06-12 11:14:01 I OnlineLossCallback(every_n_updates=50)
06-12 11:14:01 I OnlineAccuracyCallback(every_n_updates=50)
06-12 11:14:01 I OnlineAccuracyCallback(every_n_epochs=1)
06-12 11:14:01 I CheckpointCallback()
06-12 11:14:01 I CheckpointCallback(every_n_epochs=10)
06-12 11:14:01 I OfflineAccuracyCallback(every_n_epochs=1)
06-12 11:14:01 I ------------------
06-12 11:14:01 I START TRAINING
06-12 11:14:01 I initializing dataloader workers
06-12 11:14:02 I initialized dataloader workers
06-12 11:14:06 I 0 unused parameters
.../site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1536, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1536, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
06-12 11:23:44 I ------------------
06-12 11:23:44 I Epoch 1/5 (E1_U1251_S1281024)
06-12 11:23:44 I ETA: 06.12 12.02.36 estimated_duration: 00:48:35.64 time_since_last_log: 00:09:43.12 time_per_update: 00:00:00.46
06-12 11:23:44 I data=[0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,
 0.00, 0.00, 0.00, 0.00] update=[0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,
 0.45, 0.45, 0.45, 0.45]
06-12 11:23:44 I loss/online/main/E1: 1.67081880569458
06-12 11:23:44 I loss/online/total/E1: 1.67081880569458
06-12 11:23:44 I accuracy1/online/main/E1: 0.718600
06-12 11:23:54 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.21
06-12 11:23:56 I accuracy1/val/main: 0.823700
06-12 11:23:56 I loss/val/main: 0.75390625
06-12 11:33:34 I ------------------
06-12 11:33:34 I Epoch 2/5 (E2_U2502_S2562048)
06-12 11:33:34 I ETA: 06.12 12.03.03 estimated_duration: 00:39:19.15 time_since_last_log: 00:09:49.78 time_per_update: 00:00:00.47
06-12 11:33:34 I data=[0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,
 0.00, 0.00, 0.00, 0.00] update=[0.44, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.44, 0.45, 0.45, 0.45, 0.45,
 0.45, 0.45, 0.45, 0.45]
06-12 11:33:34 I loss/online/main/E1: 1.663590669631958
06-12 11:33:34 I loss/online/total/E1: 1.663590669631958
06-12 11:33:34 I accuracy1/online/main/E1: 0.719285
06-12 11:33:43 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.19
06-12 11:33:44 I accuracy1/val/main: 0.823780
06-12 11:33:44 I loss/val/main: 0.75390625
06-12 11:43:22 I ------------------
06-12 11:43:22 I Epoch 3/5 (E3_U3753_S3843072)
06-12 11:43:22 I ETA: 06.12 12.03.00 estimated_duration: 00:39:16.33 time_since_last_log: 00:09:48.37 time_per_update: 00:00:00.47
06-12 11:43:22 I data=[0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,
 0.00, 0.00, 0.00, 0.00] update=[0.44, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.44, 0.45, 0.45, 0.45, 0.45,
 0.45, 0.45, 0.45, 0.45]
06-12 11:43:22 I loss/online/main/E1: 1.6684699058532715
06-12 11:43:22 I loss/online/total/E1: 1.6684699058532715
06-12 11:43:22 I accuracy1/online/main/E1: 0.719422
06-12 11:43:32 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.19
06-12 11:43:32 I accuracy1/val/main: 0.823940
06-12 11:43:32 I loss/val/main: 0.75390625
06-12 11:53:10 I ------------------
06-12 11:53:10 I Epoch 4/5 (E4_U5004_S5124096)
06-12 11:53:10 I ETA: 06.12 12.02.58 estimated_duration: 00:39:14.28 time_since_last_log: 00:09:47.54 time_per_update: 00:00:00.46
06-12 11:53:10 I data=[0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,
 0.00, 0.00, 0.00, 0.00] update=[0.44, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.44, 0.45, 0.45, 0.45, 0.45,
 0.45, 0.45, 0.45, 0.45]
06-12 11:53:10 I loss/online/main/E1: 1.6612696647644043
06-12 11:53:10 I loss/online/total/E1: 1.6612696647644043
06-12 11:53:10 I accuracy1/online/main/E1: 0.722091
06-12 11:53:19 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.19
06-12 11:53:19 I accuracy1/val/main: 0.823920
06-12 11:53:19 I loss/val/main: 0.75
06-12 12:02:57 I ------------------
06-12 12:02:57 I Epoch 5/5 (E5_U6255_S6405120)
06-12 12:02:57 I ETA: 06.12 12.02.57 estimated_duration: 00:39:13.49 time_since_last_log: 00:09:47.78 time_per_update: 00:00:00.46
06-12 12:02:57 I data=[0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,
 0.00, 0.00, 0.00, 0.00] update=[0.44, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.44, 0.45, 0.45, 0.45, 0.45,
 0.45, 0.45, 0.45, 0.45]
06-12 12:02:57 I loss/online/main/E1: 1.6573314666748047
06-12 12:02:57 I loss/online/total/E1: 1.6573314666748047
06-12 12:02:57 I accuracy1/online/main/E1: 0.721931
06-12 12:03:07 I profiling/offline_accuracy_callback/val.x.class: data=0.00 forward=0.20
06-12 12:03:07 I accuracy1/val/main: 0.824120
06-12 12:03:07 I loss/val/main: 0.75390625
06-12 12:03:07 I ------------------
06-12 12:03:07 I AFTER TRAINING
06-12 12:03:08 I ------------------
06-12 12:03:08 I total_train_data_time:   [2.03, 1.93, 1.88, 2.40, 2.09, 1.71, 1.94, 1.75, 2.95, 2.86, 2.96, 2.66,
 2.74, 2.95, 2.58, 2.81]
06-12 12:03:08 I total_update_time: [2782.64, 2789.73, 2794.13, 2790.10, 2791.87, 2789.87, 2788.47, 2786.04,
 2793.31, 2791.43, 2790.85, 2792.19, 2792.39, 2791.07, 2791.43, 2792.01]
06-12 12:03:08 I saved vislstm to .../in1k/6x5zxnr9/checkpoints/vislstm cp=last model.th
06-12 12:03:08 I saved trainer state_dict to .../in1k/6x5zxnr9/checkpoints/trainer cp=last.th
06-12 12:03:08 I saved vislstm to .../in1k/6x5zxnr9/checkpoints/vislstm cp=last model.th
06-12 12:03:08 I saved vislstm to .../in1k/6x5zxnr9/checkpoints/vislstm cp=latest model.th
06-12 12:03:09 I saved vislstm optim to .../in1k/6x5zxnr9/checkpoints/vislstm cp=latest optim.th
06-12 12:03:09 I saved trainer state_dict to .../in1k/6x5zxnr9/checkpoints/trainer cp=last.th
06-12 12:03:09 I saved trainer state_dict to .../in1k/6x5zxnr9/checkpoints/trainer cp=latest.th
06-12 12:03:09 I ------------------
06-12 12:03:09 I offline_accuracy_callback dataset_key=val.x.class
06-12 12:03:09 I total_data_time:    [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,
 0.01, 0.01, 0.01, 0.01]
06-12 12:03:09 I total_forward_time: [0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.02, 1.02, 1.02, 1.02,
 1.02, 1.02, 1.02, 1.02]
06-12 12:03:09 I writing 131 log entries to .../in1k/6x5zxnr9/primitive/entries.th
06-12 12:03:09 I ------------------
06-12 12:03:09 I summarize logvalues
06-12 12:03:09 I loss/online/main/U50/min: 1.6197277307510376
06-12 12:03:09 I loss/online/total/U50/min: 1.6197277307510376
06-12 12:03:09 I accuracy1/online/main/U50/max: 0.731992244720459
06-12 12:03:09 I loss/online/main/E1/min: 1.6573314666748047
06-12 12:03:09 I loss/online/total/E1/min: 1.6573314666748047
06-12 12:03:09 I accuracy1/online/main/E1/max: 0.7220910787582397
06-12 12:03:09 I accuracy1/val/main/max: 0.8241199851036072
06-12 12:03:09 I loss/val/main/min: 0.75
06-12 12:03:09 I pushing summarized logvalues to wandb
06-12 12:03:09 W cuda profiling is not activated -> all cuda calls are executed asynchronously -> this will result in inaccurate profiling times where the time for all asynchronous cuda operation will be attributed to the first synchronous cuda operation https://github.com/BenediktAlkin/KappaProfiler?tab=readme-ov-file#time-async-operations
06-12 12:03:09 I full profiling times:
  2948.98 train
     0.00 train.DatasetStatsCallback.before_training
     0.00 train.ParamCountCallback.before_training
     0.09 train.CopyPreviousConfigCallback.before_training
     0.01 train.CopyPreviousSummaryCallback.before_training
     0.00 train.ProgressCallback(every_n_epochs=1).before_training
     0.00 train.OnlineAccuracyCallback(every_n_updates=50).before_training
     0.00 train.OnlineAccuracyCallback(every_n_epochs=1).before_training
     0.00 train.CheckpointCallback().before_training
     0.00 train.CheckpointCallback(every_n_epochs=10).before_training
     0.00 train.OfflineAccuracyCallback(every_n_epochs=1).before_training
     1.17 train.iterator
     2.03 train.data_loading
  2782.64 train.update
     0.06 train.OnlineLossCallback(every_n_epochs=1).track_after_accumulation_step
     0.03 train.OnlineLossCallback(every_n_updates=50).track_after_accumulation_step
    78.80 train.OnlineAccuracyCallback(every_n_updates=50).track_after_accumulation_step
     1.86 train.OnlineAccuracyCallback(every_n_epochs=1).track_after_accumulation_step
     0.03 train.TrainTimeCallback(every_n_epochs=1).track_after_update_step
     0.00 train.LrCallback(every_n_updates=50).after_update
     0.00 train.FreezerCallback(every_n_updates=50).after_update
     0.56 train.OnlineLossCallback(every_n_updates=50).after_update
     0.04 train.OnlineAccuracyCallback(every_n_updates=50).after_update
     0.00 train.ProgressCallback(every_n_epochs=1).after_epoch
     0.02 train.TrainTimeCallback(every_n_epochs=1).after_epoch
     0.03 train.OnlineLossCallback(every_n_epochs=1).after_epoch
     0.02 train.OnlineAccuracyCallback(every_n_epochs=1).after_epoch
    50.77 train.OfflineAccuracyCallback(every_n_epochs=1).after_epoch
     0.08 train.TrainTimeCallback(every_n_epochs=1).after_training
     0.31 train.CheckpointCallback().after_training
     1.24 train.CheckpointCallback(every_n_epochs=10).after_training
06-12 12:03:09 I ------------------
06-12 12:03:09 I CLEANUP
06-12 12:03:09 I ------------------
06-12 12:03:09 I encountered 1 warnings
06-12 12:03:09 I encountered 0 errors